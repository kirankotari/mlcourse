{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\" />\n",
    "    \n",
    "## [mlcourse.ai](https://mlcourse.ai) â€“ Open Machine Learning Course \n",
    "### <center> Author: Name as in the rating, ODS Slack nickname\n",
    "    \n",
    "## <center> Tutorial\n",
    "### <center> \"Your topic\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
<<<<<<< HEAD
    "**Rules:**\n",
    " - It must be a tutorial (reproducible Jupyter notebook, ipynb file), i.e you should teach some skills, do not just express your ideas on some theoretical concept\n",
    " - It is not just a translation of somebody else's material. You can borrow some stuff, but with fair links/citations\n",
    " - The prerequisite for reading your tutorial should be basic ML as taught in [mlcourse.ai](https://mlcourse.ai). Do not write in-depth articles about neural nets, probabilistic programming, Bayesian approach, reinforcement learning etc. - topics like these need a thorough approach. That is, sure, you can write articles like that, but not in the format of [mlcourse.ai](https://mlcourse.ai) tutorials. On the other hand, it's hardly worth making a tutorial too simple (e.g., to take some library and demonstrate only a couple of methods from it)\n",
    " - A typical tutorial shall be 30-60 minutes to read and digest (however, here exceptions are possible)\n",
    " - Check out a [list](https://github.com/Yorko/mlcourse.ai/wiki/Individual-projects-and-tutorials-(in-Russian)) of published tutorials from previous runs of this course (in Russian). Yes, it's in Russian but Google Translate can kind of give you an insight, and topic that are already covered. For those who already passed the course: definitely, translating somebody else's (or your own) tutorial into English is not going to work\n",
=======
    "## Rules\n",
    "Read carefully the [roadmap](https://mlcourse.ai/roadmap)!\n",
    "This time you are asked to publish a tutorial as a Kaggle Kernel in [mlcourse.ai Dataset](https://www.kaggle.com/kashnitsky/mlcourse).\n",
>>>>>>> 6a6e3a7ca926a4fe7c956f1186ce57a8697a13b3
    "\n",
    "\n",
    "## Exemplar topics  \n",
    "Here are a dozen of exemplar topics (but this is a just as an example, you can/should come up with your own):\n",
    "\n",
    "#### Pandas & Data Analysis\n",
    "- Data collection, crawling, working with XML, JSON etc.\n",
    "- Working with big DataFrames, Dask. \n",
    "- Optimizing memory usage while working with NumPy, Pandas and pure Python\n",
    "- Working with JSON and XML in Pandas\n",
    "- When to use SQL with Pandas or just SQL\n",
    "- Feather data format\n",
    "- Reducing memory consumption while analyzing data: tips & tricks\n",
    "- Data analysis with bash (utilizing extremely efficient command line utils)\n",
    " \n",
    "#### Visualization\n",
    " - Overview of Bokeh or another viz library\n",
    " - Working with geo-spatial data in Python\n",
    " - Creating animations for data analysis\n",
    " - Tree visualization in Python (smth more than standard graphviz)\n",
    " - Dimentionality reduction for visualization (there are some methods besides t-SNE, e.g. UMAP)\n",
    " \n",
    "#### Decision trees & kNN\n",
    " - Decision trees with statistical tests in the nodes\n",
    " - Overview of H2O library\n",
    " - kNN in production systems: Annoy \n",
    " - kNN as a strong baseline in recommender systems\n",
    " \n",
    "#### Linear models\n",
    "\n",
    " - Poisson/quantile or another type of regression\n",
    " - Multi-label classification\n",
    " - Efficient implementation of linear models (based QR-decomposition or similar)\n",
    " - Median and quantile regression\n",
    " - Interpreting linear models: SHAP, eli5\n",
    "\n",
    "#### Features & Validation\n",
    " - Counters in supervised learning tasks: WOE, smoothed likelihood and other methods of feature engineering based on the target feature\n",
    " - there is much more in Scikit-learn that we didn't cover (NestedCV, GroupKFold etc)\n",
    "\n",
    "#### Bagging and Random Forest\n",
    " - Interpreting RF by reducing forests to trees\n",
    " - Compressing random forests\n",
    " - Various heuristics for assessing feature importance in forests and boosting\n",
    " \n",
    "#### Unsupervised learning\n",
    " - Review of some clustering method (with motivation, why it is needed)\n",
    " - Efficient PCA implementation, Randomized PCA, online PCA\n",
    " - Unsupervised learning in real tasks\n",
    " - Word2Vec applied to sequential data (even website sessions like in the \"Alice\" competition)\n",
    "\n",
    "#### SGD & Vowpal Wabbit\n",
    " - Something that better covers the course material, e.g. a broader review of Vowpal Wabbit \n",
    " - Matrix factorization, FMs and Vowpal Wabbit implementation\n",
    " - FTRL algorithm: Follow the regularized leader\n",
    "\n",
    "#### Time series\n",
    " - Predicting multiple time series at the same time\n",
    " - Detecting anomalies in time series\n",
    " \n",
    "#### Boosting\n",
    " - CatBoost overview + examples where it works really well \n",
    " - Overview of the H2O library\n",
    " - Gradient boosting and GPU speedup\n",
    "\n",
    "#### Other\n",
    " - A/B tests and interleaving\n",
    " - Bayesian methods of hyperparameter optimization\n",
    " - Versioning datasets (ex. DVC.org)\n",
    " - Optimizing non-trivial metrics: tips & tricks\n",
    " - Closer to business metrics: uplift modelling (pylift)"
   ]
<<<<<<< HEAD
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
=======
    "## Rules\n",
    "Read carefully the [roadmap](https://mlcourse.ai/roadmap)!\n",
    "This time you are asked to publish a tutorial as a Kaggle Kernel in [mlcourse.ai Dataset](https://www.kaggle.com/kashnitsky/mlcourse).\n",
    "\n",
    "\n",
    "## Exemplar topics  \n",
    "Here are a dozen of exemplar topics (but this is a just as an example, you can/should come up with your own):\n",
    "\n",
    "#### Pandas & Data Analysis\n",
    "- Data collection, crawling, working with XML, JSON etc.\n",
    "- Working with big DataFrames, Dask. \n",
    "- Optimizing memory usage while working with NumPy, Pandas and pure Python\n",
    "- Working with JSON and XML in Pandas\n",
    "- When to use SQL with Pandas or just SQL\n",
    "- Feather data format\n",
    "- Reducing memory consumption while analyzing data: tips & tricks\n",
    "- Data analysis with bash (utilizing extremely efficient command line utils)\n",
    " \n",
    "#### Visualization\n",
    " - Overview of Bokeh or another viz library\n",
    " - Working with geo-spatial data in Python\n",
    " - Creating animations for data analysis\n",
    " - Tree visualization in Python (smth more than standard graphviz)\n",
    " - Dimentionality reduction for visualization (there are some methods besides t-SNE, e.g. UMAP)\n",
    " \n",
    "#### Decision trees & kNN\n",
    " - Decision trees with statistical tests in the nodes\n",
    " - Overview of H2O library\n",
    " - kNN in production systems: Annoy \n",
    " - kNN as a strong baseline in recommender systems\n",
    " \n",
    "#### Linear models\n",
    "\n",
    " - Poisson/quantile or another type of regression\n",
    " - Multi-label classification\n",
    " - Efficient implementation of linear models (based QR-decomposition or similar)\n",
    " - Median and quantile regression\n",
    " - Interpreting linear models: SHAP, eli5\n",
    "\n",
    "#### Features & Validation\n",
    " - Counters in supervised learning tasks: WOE, smoothed likelihood and other methods of feature engineering based on the target feature\n",
    " - there is much more in Scikit-learn that we didn't cover (NestedCV, GroupKFold etc)\n",
    "\n",
    "#### Bagging and Random Forest\n",
    " - Interpreting RF by reducing forests to trees\n",
    " - Compressing random forests\n",
    " - Various heuristics for assessing feature importance in forests and boosting\n",
    " \n",
    "#### Unsupervised learning\n",
    " - Review of some clustering method (with motivation, why it is needed)\n",
    " - Efficient PCA implementation, Randomized PCA, online PCA\n",
    " - Unsupervised learning in real tasks\n",
    " - Word2Vec applied to sequential data (even website sessions like in the \"Alice\" competition)\n",
    "\n",
    "#### SGD & Vowpal Wabbit\n",
    " - Something that better covers the course material, e.g. a broader review of Vowpal Wabbit \n",
    " - Matrix factorization, FMs and Vowpal Wabbit implementation\n",
    " - FTRL algorithm: Follow the regularized leader\n",
    "\n",
    "#### Time series\n",
    " - Predicting multiple time series at the same time\n",
    " - Detecting anomalies in time series\n",
    " \n",
    "#### Boosting\n",
    " - CatBoost overview + examples where it works really well \n",
    " - Overview of the H2O library\n",
    " - Gradient boosting and GPU speedup\n",
    "\n",
    "#### Other\n",
    " - A/B tests and interleaving\n",
    " - Bayesian methods of hyperparameter optimization\n",
    " - Versioning datasets (ex. DVC.org)\n",
    " - Optimizing non-trivial metrics: tips & tricks\n",
    " - Closer to business metrics: uplift modelling (pylift)"
   ]
>>>>>>> ec8ebec14b4b3ceea0972a86b7885ff3b15d207d
=======
>>>>>>> 6a6e3a7ca926a4fe7c956f1186ce57a8697a13b3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
<<<<<<< HEAD
   "version": "3.6.6"
=======
   "version": "3.7.1"
>>>>>>> ec8ebec14b4b3ceea0972a86b7885ff3b15d207d
=======
   "version": "3.7.1"
>>>>>>> 6a6e3a7ca926a4fe7c956f1186ce57a8697a13b3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
